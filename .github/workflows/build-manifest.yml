name: Build photos (resize + manifest)

permissions:
  contents: write

on:
  push:
    paths:
      - 'img/originals/*.jpg'
      - 'img/originals/*.jpeg'
      - 'img/originals/*.JPG'
      - 'img/originals/*.JPEG'
      - '.github/workflows/build-manifest.yml'
  workflow_dispatch:

concurrency:
  group: build-photos
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      SHEET_CSV_URL: ${{ secrets.SHEET_CSV_URL }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          persist-credentials: true
          fetch-depth: 0

      - name: Install ImageMagick
        run: |
          sudo apt-get update
          sudo apt-get install -y imagemagick

      - name: Ensure ImageMagick is available
        run: convert -version

      - name: Resize originals into 640/1280/2560
        shell: bash
        run: |
          set -euo pipefail
          shopt -s nullglob nocaseglob
          mkdir -p img img/originals
          for f in img/originals/*.{jpg,jpeg,JPG,JPEG}; do
            [ -e "$f" ] || continue
            stem="$(basename "${f%.*}")"
            for size in 640 1280 2560; do
              out="img/${stem}-${size}.jpg"
              if [[ ! -f "$out" ]]; then
                convert "$f" -auto-orient -strip -resize "${size}x${size}>" -quality 88 "$out"
                echo "Wrote $out"
              fi
            done
          done

      - name: Build img/photos.json from originals (newest first)
        shell: bash
        run: |
          set -euo pipefail
          shopt -s nullglob nocaseglob

          # Collect "<ISO8601> <stem>" from originals and sort newest-first
          mapfile -t LINES < <(
            for f in img/originals/*.{jpg,jpeg,JPG,JPEG}; do
              stem="$(basename "${f%.*}")"
              ts="$(git log -1 --format=%cI -- "$f" 2>/dev/null || echo 1970-01-01T00:00:00Z)"
              echo "$ts $stem"
            done | LC_ALL=C sort -r
          )

          # De-duplicate by stem, keeping newest
          mapfile -t UNIQUE < <(printf "%s\n" "${LINES[@]}" | awk '!seen[$2]++')

          : > img/photos.json
          echo "[" >> img/photos.json
          first=1
          for line in "${UNIQUE[@]}"; do
            ts="${line%% *}"
            b="${line#* }"
            if [[ -f "img/${b}-640.jpg" && -f "img/${b}-1280.jpg" && -f "img/${b}-2560.jpg" ]]; then
              date_short="${ts%%T*}"  # commit date (YYYY-MM-DD)
              if [[ $first -eq 0 ]]; then echo "," >> img/photos.json; fi
              first=0
              printf '  {"base":"%s","title":"%s","overlay":"%s","alt":"%s","date":"%s","ts":"%s"}' \
                "$b" "$b" "$b" "$b" "$date_short" "$ts" >> img/photos.json
            fi
          done
          echo ""  >> img/photos.json
          echo "]" >> img/photos.json

      - name: Merge CSV metadata (adds `taken`, camera, lens, film, location, tags)
        if: env.SHEET_CSV_URL != ''
        shell: bash
        run: |
          set -euo pipefail
          python3 - <<'PY'
          import csv, io, json, os, sys, urllib.request

          photos_path = "img/photos.json"

          # Load current photos.json
          with open(photos_path, "r", encoding="utf-8") as f:
            photos = json.load(f)
          by_base = {p["base"]: p for p in photos}

          # Download CSV
          url = os.environ.get("SHEET_CSV_URL", "").strip()
          if not url:
            print("SHEET_CSV_URL not set; skipping CSV merge.")
            sys.exit(0)

          with urllib.request.urlopen(url) as resp:
            content = resp.read().decode("utf-8")

          # Read CSV with flexible headers
          reader = csv.DictReader(io.StringIO(content))
          def norm(h):
            return h.strip().lower()

          # header normalization map
          # supports "date_taken (YYYY-MM-DD)" or "date_taken" etc.
          header_map = {}
          for h in reader.fieldnames or []:
            hl = norm(h)
            if hl.startswith("date_taken"):
              header_map[h] = "date_taken"
            elif hl.startswith("tags"):
              header_map[h] = "tags"
            else:
              header_map[h] = hl  # base, title, camera, lens, film, location, overlay, alt

          for row in reader:
            # normalize keys
            r = { header_map[k]: (row[k] or "").strip() for k in row }

            base = r.get("base","").strip()
            if not base or base not in by_base:
              continue

            p = by_base[base]

            # Simple fields (if provided)
            for key in ["title","overlay","alt","camera","lens","film","location"]:
              val = r.get(key,"").strip()
              if val:
                p[key] = val

            # tags: comma separated -> array
            tags = r.get("tags","").strip()
            if tags:
              p["tags"] = [t.strip() for t in tags.split(",") if t.strip()]
            else:
              p.setdefault("tags", [])

            # date_taken -> taken (fallback to commit date)
            shot = r.get("date_taken","").strip()
            if shot:
              p["taken"] = shot
            else:
              p["taken"] = p.get("date")  # fallback

          # Write back
          with open(photos_path, "w", encoding="utf-8") as f:
            json.dump(photos, f, ensure_ascii=False, indent=2)
            f.write("\n")
          print("Merged CSV metadata into img/photos.json (added `taken`).")
          PY

      - name: Commit and push generated files safely
        shell: bash
        run: |
          set -euo pipefail
          git config user.name  "actions-bot"
          git config user.email "actions@users.noreply.github.com"

          git pull --rebase --autostash

          git add img/*.jpg img/photos.json 2>/dev/null || true
          if git diff --cached --quiet; then
            echo "No changes to commit."
            exit 0
          fi

          git commit -m "Resize photos + update manifest (CSV merged)"
          if ! git push; then
            git pull --rebase --autostash
            git push
          fi
